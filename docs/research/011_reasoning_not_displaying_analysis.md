# 011. Reasoning UI ë¯¸í‘œì‹œ ë¬¸ì œ ë¶„ì„

## 1. ë¬¸ì œ í˜„ìƒ

Chat UIì—ì„œ GPT-5 ëª¨ë¸ ì‚¬ìš© ì‹œ **Reasoning(ì¶”ë¡  ê³¼ì •) UIê°€ í‘œì‹œë˜ì§€ ì•ŠìŒ**.

- `ReasoningRenderer` ì»´í¬ë„ŒíŠ¸ê°€ ì¡´ì¬í•˜ì§€ë§Œ ë Œë”ë§ë˜ì§€ ì•ŠìŒ
- ìŠ¤íŠ¸ë¦¬ë° ì¤‘ì—ë„ reasoning ë‚´ìš©ì´ ë³´ì´ì§€ ì•ŠìŒ

---

## 2. í˜„ì¬ ì•„í‚¤í…ì²˜

### 2.1 ë°ì´í„° íë¦„

```
[í”„ë¡ íŠ¸ì—”ë“œ]                     [ë°±ì—”ë“œ]                        [OpenAI]
     â”‚                              â”‚                              â”‚
     â”‚  createConversation()        â”‚                              â”‚
     â”‚  appendMessage()             â”‚                              â”‚
     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚                              â”‚
     â”‚                              â”‚   responses.create()         â”‚
     â”‚                              â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚
     â”‚                              â”‚                              â”‚
     â”‚                              â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
     â”‚                              â”‚   Response (with reasoning)  â”‚
     â”‚                              â”‚                              â”‚
     â”‚  SSE: EventSource            â”‚                              â”‚
     â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                              â”‚
     â”‚  AgentEvent stream           â”‚                              â”‚
```

### 2.2 ê´€ë ¨ íŒŒì¼

| ìœ„ì¹˜ | íŒŒì¼ | ì—­í•  |
|------|------|------|
| Frontend | `hooks/useAgentStream.ts` | SSEë¡œ ì´ë²¤íŠ¸ ìˆ˜ì‹  |
| Frontend | `hooks/useMultiTabChat.ts` | ì´ë²¤íŠ¸ë¥¼ ChatTurnìœ¼ë¡œ ë³€í™˜ |
| Frontend | `lib/chatRenderUtils.ts` | Turnì„ RenderItemìœ¼ë¡œ ë³€í™˜ |
| Frontend | `components/chat/renderers/ReasoningRenderer.tsx` | Reasoning UI ë Œë”ë§ |
| Backend | `agent/core/llm/providers.py` | GPT-5 API í˜¸ì¶œ |
| Backend | `agent/core/deep/event_mapper.py` | LangChain ì½œë°± â†’ AgentEvent ë³€í™˜ |
| Backend | `agent/core/orchestrator.py` | Agent ì‹¤í–‰ ë° ì´ë²¤íŠ¸ emit |
| Backend | `app/api/v1/agent/router.py` | SSE ì—”ë“œí¬ì¸íŠ¸ |

---

## 3. ì›ì¸ ë¶„ì„

### 3.1 í”„ë¡ íŠ¸ì—”ë“œ - Reasoning í‘œì‹œ ì¡°ê±´

**íŒŒì¼:** `lib/chatRenderUtils.ts:69-82`

```typescript
// Reasoning (ì¡´ì¬í•˜ê±°ë‚˜ ìŠ¤íŠ¸ë¦¬ë° ì¤‘ì´ë©´)
if (turn.reasoningText || isActive) {
  const item: ReasoningItem = {
    id: `reasoning-${baseRunId || turn.key}`,
    type: 'reasoning',
    // ...
    content: turn.reasoningText || '',
  };
  items.push(item);
}
```

**íŒŒì¼:** `hooks/useMultiTabChat.ts:434-442`

```typescript
// Extract reasoning text
turn.reasoningText = turn.events
  .filter(event => event.type === 'reasoning')
  .map(event => {
    const content = event.content as any;
    return content && typeof content === 'object' && content.reason
      ? String(content.reason)
      : '';
  })
  .filter(Boolean)
  .join('\n\n');
```

**ë¬¸ì œì :** í”„ë¡ íŠ¸ì—”ë“œëŠ” `content.reason` í•„ë“œë¥¼ ì°¾ì§€ë§Œ, ë°±ì—”ë“œëŠ” ì´ í•„ë“œë¥¼ ë³´ë‚´ì§€ ì•ŠìŒ.

### 3.2 ë°±ì—”ë“œ - í˜„ì¬ Reasoning ì´ë²¤íŠ¸ êµ¬ì¡°

**íŒŒì¼:** `agent/core/deep/event_mapper.py`

```python
async def on_llm_start(self, *args: Any, **kwargs: Any) -> None:
    await self._emit(
        AgentEvent(
            type=EventType.REASONING,
            subtype=EventSubType.START,
            content={"phase": "llm_start"},  # âŒ ì‹¤ì œ reasoning ë‚´ìš© ì—†ìŒ
            metadata={"run_id": self._run_id},
        )
    )

async def on_llm_end(self, response: Any, **kwargs: Any) -> None:
    text = None
    try:
        gens = getattr(response, "generations", None) or []
        if gens and gens[0]:
            msg = getattr(gens[0][0], "message", None)
            text = getattr(msg, "content", None)  # âŒ ì´ê±´ ìµœì¢… ì‘ë‹µ, reasoningì´ ì•„ë‹˜
    except Exception:
        text = None
    await self._emit(
        AgentEvent(
            type=EventType.REASONING,
            subtype=EventSubType.CHUNK,
            content={"phase": "llm_end", "text": text},  # âŒ "reason" í•„ë“œ ì•„ë‹˜
        )
    )
```

**ë¬¸ì œì :**
1. `on_llm_start`ëŠ” `{"phase": "llm_start"}`ë§Œ ì „ì†¡ - ì‹¤ì œ reasoning ë‚´ìš© ì—†ìŒ
2. `on_llm_end`ëŠ” ìµœì¢… ì‘ë‹µ í…ìŠ¤íŠ¸ë¥¼ `text` í•„ë“œë¡œ ì „ì†¡ - reasoningì´ ì•„ë‹˜
3. í”„ë¡ íŠ¸ì—”ë“œê°€ ê¸°ëŒ€í•˜ëŠ” `reason` í•„ë“œê°€ ì—†ìŒ

### 3.3 ë°±ì—”ë“œ - GPT-5 ì‘ë‹µ ì²˜ë¦¬ (providers.py - ë¯¸ì‚¬ìš©)

**íŒŒì¼:** `agent/core/llm/providers.py:53-79`

```python
async def ainvoke(self, prompt: str, *, metadata: Optional[Dict[str, Any]] = None) -> str:
    config = get_settings().agent
    request_kwargs: Dict[str, Any] = {
        "model": self._model,
        "input": prompt,
    }

    if _is_gpt5_model(self._model):
        request_kwargs["reasoning"] = {"effort": config.reasoning_effort}  # âœ… reasoning ìš”ì²­
        request_kwargs["text"] = {"verbosity": config.text_verbosity}

    response = await self._client.responses.create(**request_kwargs)

    if hasattr(response, "output_text"):
        return response.output_text  # âŒ output_textë§Œ ë°˜í™˜, reasoning ë¬´ì‹œ!

    # Fallbackë„ reasoningì„ ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ
    if response.output:
        parts = []
        for item in response.output:
            if getattr(item, "content", None):
                for content in item.content:
                    if getattr(content, "text", None):
                        parts.append(content.text)
        if parts:
            return "\n".join(parts)
    return ""
```

**âš ï¸ ì¤‘ìš”:** ì´ ì½”ë“œëŠ” ì‹¤ì œ ì—ì´ì „íŠ¸ì—ì„œ **ì‚¬ìš©ë˜ì§€ ì•ŠìŒ** (3.4 ì°¸ì¡°)

---

### 3.4 ğŸ”´ ê·¼ë³¸ ì›ì¸ ë°œê²¬ (2026-01-16 ê²€ì¦)

**ì‹¤ì œ ì—ì´ì „íŠ¸ëŠ” `providers.py`ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  `langchain_openai.ChatOpenAI`ë¥¼ ì§ì ‘ ì‚¬ìš©!**

**íŒŒì¼:** `agent/core/deep/agent.py:144-148`

```python
from langchain_openai import ChatOpenAI

chat_model = ChatOpenAI(
    model=effective_model,
    api_key=effective_api_key,
    base_url=str(settings.agent.api_base) if settings.agent.api_base else None,
)
```

**ê²€ì¦ ë°©ë²•:** ë°±ì—”ë“œ ë¡œê·¸ì—ì„œ í™•ì¸

```
# ì‹¤ì œ í˜¸ì¶œë˜ëŠ” API (Chat Completions API)
httpx | HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"

# providers.pyê°€ ì‚¬ìš©í–ˆë‹¤ë©´ ì´ë ‡ê²Œ ë³´ì—¬ì•¼ í•¨ (Responses API)
# httpx | HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
```

**API ë¹„êµ:**

| êµ¬ë¶„ | providers.py | agent.py (ì‹¤ì œ ì‚¬ìš©) |
|------|-------------|---------------------|
| LLM í´ë˜ìŠ¤ | `OpenAILLMProvider` | `ChatOpenAI` (LangChain) |
| API ì—”ë“œí¬ì¸íŠ¸ | `/v1/responses` | `/v1/chat/completions` |
| Reasoning ì§€ì› | âœ… ê°€ëŠ¥ | âŒ ë¶ˆê°€ëŠ¥ |

**ê²°ë¡ :** LangChainì˜ `ChatOpenAI`ëŠ” Chat Completions APIë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ, GPT-5ì˜ Responses API reasoning ê¸°ëŠ¥ì„ **êµ¬ì¡°ì ìœ¼ë¡œ ì§€ì›í•  ìˆ˜ ì—†ìŒ**.

---

## 4. GPT-5 Responses API êµ¬ì¡°

### 4.1 ìš”ì²­ ì‹œ reasoning ì„¤ì •

```python
request_kwargs = {
    "model": "gpt-5",
    "input": prompt,
    "reasoning": {
        "effort": "medium",      # none, minimal, low, medium, high, xhigh
        "summary": "concise"     # auto, concise, detailed
    }
}
```

### 4.2 ì‘ë‹µ êµ¬ì¡°

```python
response = {
    "id": "resp_xxx",
    "object": "response",
    "status": "completed",
    "output": [
        # Reasoning ë¸”ë¡ (type: "reasoning")
        {
            "type": "reasoning",
            "id": "reasoning_xxx",
            "status": "completed",
            "content": [
                {
                    "type": "text",
                    "text": "Let me think about this step by step..."
                }
            ],
            "summary": [
                {
                    "type": "text",
                    "text": "Analyzed the problem and determined..."
                }
            ]
        },
        # ìµœì¢… ì‘ë‹µ ë¸”ë¡ (type: "message")
        {
            "type": "message",
            "role": "assistant",
            "content": [
                {
                    "type": "text",
                    "text": "Here is my answer..."
                }
            ]
        }
    ],
    "output_text": "Here is my answer...",  # ìµœì¢… í…ìŠ¤íŠ¸ë§Œ í¬í•¨
    "usage": {
        "input_tokens": 100,
        "output_tokens": 500,
        "reasoning_tokens": 200  # reasoningì— ì‚¬ìš©ëœ í† í°
    }
}
```

### 4.3 output ë°°ì—´ì˜ ê°€ëŠ¥í•œ íƒ€ì…ë“¤

| type | ì„¤ëª… |
|------|------|
| `message` | ëª¨ë¸ì˜ ìµœì¢… ì‘ë‹µ ë©”ì‹œì§€ |
| `reasoning` | ì¶”ë¡  ê³¼ì • (chain of thought) |
| `function_call` | í•¨ìˆ˜ í˜¸ì¶œ |
| `file_search` | íŒŒì¼ ê²€ìƒ‰ ë„êµ¬ í˜¸ì¶œ |
| `web_search` | ì›¹ ê²€ìƒ‰ ë„êµ¬ í˜¸ì¶œ |
| `code_interpreter` | ì½”ë“œ ì¸í„°í”„ë¦¬í„° í˜¸ì¶œ |

### 4.4 Reasoning ê°ì²´ ìƒì„¸ êµ¬ì¡°

```typescript
interface ReasoningOutput {
  type: "reasoning";
  id: string;
  status: "in_progress" | "completed" | "incomplete";

  // ì‹¤ì œ reasoning í…ìŠ¤íŠ¸
  content: Array<{
    type: "text";
    text: string;
  }>;

  // reasoning ìš”ì•½ (summary ì˜µì…˜ ì„¤ì • ì‹œ)
  summary: Array<{
    type: "text";
    text: string;
  }>;

  // ì•”í˜¸í™”ëœ ì»¨í…ì¸  (include íŒŒë¼ë¯¸í„°ë¡œ ìš”ì²­ ì‹œ)
  encrypted_content?: string;
}
```

---

## 5. í•´ê²° ë°©ì•ˆ

### 5.0 ğŸ”´ í•´ê²° ë°©ì•ˆ ì¬ê²€í†  í•„ìš” (2026-01-16)

ê¸°ì¡´ í•´ê²° ë°©ì•ˆ(5.1~5.3)ì€ `providers.py`ê°€ ì‚¬ìš©ëœë‹¤ëŠ” ê°€ì • í•˜ì— ì‘ì„±ë¨.
ì‹¤ì œë¡œëŠ” `ChatOpenAI`ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ **ë‹¤ë¥¸ ì ‘ê·¼ í•„ìš”**.

#### ì˜µì…˜ A: Custom LangChain LLM Wrapper êµ¬í˜„
- `BaseChatModel`ì„ ìƒì†í•˜ì—¬ Responses APIë¥¼ í˜¸ì¶œí•˜ëŠ” ì»¤ìŠ¤í…€ í´ë˜ìŠ¤ ìƒì„±
- ì¥ì : LangGraph/LangChain ìƒíƒœê³„ì™€ í˜¸í™˜
- ë‹¨ì : ë³µì¡ë„ ë†’ìŒ, tool calling ë“± ê¸°ëŠ¥ ì¬êµ¬í˜„ í•„ìš”

#### ì˜µì…˜ B: LangChainì˜ Responses API ì§€ì› ëŒ€ê¸°/í™•ì¸
- `langchain-openai` íŒ¨í‚¤ì§€ê°€ Responses APIë¥¼ ì§€ì›í•˜ëŠ”ì§€ í™•ì¸
- ìµœì‹  ë²„ì „ì—ì„œ `ChatOpenAI`ì— reasoning ì˜µì…˜ì´ ìˆëŠ”ì§€ ì¡°ì‚¬

#### ì˜µì…˜ C: ì—ì´ì „íŠ¸ ì•„í‚¤í…ì²˜ ë³€ê²½
- LangChain ëŒ€ì‹  ì§ì ‘ OpenAI Responses API ì‚¬ìš©
- ì¥ì : ì™„ì „í•œ ì œì–´ ê°€ëŠ¥
- ë‹¨ì : LangGraphì˜ ì¥ì (ìƒíƒœ ê´€ë¦¬, ê·¸ë˜í”„ ê¸°ë°˜ ì›Œí¬í”Œë¡œìš°) í¬ê¸°

#### ì˜µì…˜ D: í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼
- ì—ì´ì „íŠ¸ ì‹¤í–‰ì€ LangChain ìœ ì§€
- Reasoning í‘œì‹œìš©ìœ¼ë¡œ ë³„ë„ API í˜¸ì¶œ (UX ê°œì„ ìš©)

**ê¶Œì¥:** ì˜µì…˜ B ë¨¼ì € ì¡°ì‚¬ í›„, í•„ìš”ì‹œ ì˜µì…˜ A ë˜ëŠ” D ì§„í–‰

---

### 5.1 (ê¸°ì¡´ ë°©ì•ˆ - providers.py ì‚¬ìš© ì‹œ) ë°±ì—”ë“œ ìˆ˜ì • - providers.py

âš ï¸ **ì£¼ì˜:** ì´ ë°©ì•ˆì€ `providers.py`ê°€ ì‹¤ì œë¡œ ì‚¬ìš©ë  ë•Œë§Œ ì ìš© ê°€ëŠ¥

GPT-5 ì‘ë‹µì—ì„œ reasoning ë¸”ë¡ì„ ì¶”ì¶œí•˜ì—¬ ë³„ë„ë¡œ ë°˜í™˜:

```python
# providers.py

from dataclasses import dataclass
from typing import Optional, List

@dataclass
class GPT5Response:
    """GPT-5 ì‘ë‹µ êµ¬ì¡°"""
    text: str
    reasoning_content: Optional[str] = None
    reasoning_summary: Optional[str] = None


class OpenAILLMProvider(BaseLLMProvider):

    async def ainvoke_with_reasoning(
        self,
        prompt: str,
        *,
        metadata: Optional[Dict[str, Any]] = None
    ) -> GPT5Response:
        """GPT-5 í˜¸ì¶œ ì‹œ reasoning í¬í•¨í•˜ì—¬ ë°˜í™˜"""
        config = get_settings().agent
        request_kwargs: Dict[str, Any] = {
            "model": self._model,
            "input": prompt,
        }

        if _is_gpt5_model(self._model):
            request_kwargs["reasoning"] = {
                "effort": config.reasoning_effort,
                "summary": "concise"  # reasoning ìš”ì•½ í™œì„±í™”
            }
            request_kwargs["text"] = {"verbosity": config.text_verbosity}

        response = await self._client.responses.create(**request_kwargs)

        # reasoning ì¶”ì¶œ
        reasoning_content = None
        reasoning_summary = None
        final_text = ""

        if response.output:
            for item in response.output:
                item_type = getattr(item, "type", None)

                if item_type == "reasoning":
                    # reasoning content ì¶”ì¶œ
                    if hasattr(item, "content") and item.content:
                        reasoning_parts = []
                        for content in item.content:
                            if getattr(content, "text", None):
                                reasoning_parts.append(content.text)
                        if reasoning_parts:
                            reasoning_content = "\n".join(reasoning_parts)

                    # reasoning summary ì¶”ì¶œ
                    if hasattr(item, "summary") and item.summary:
                        summary_parts = []
                        for summary in item.summary:
                            if getattr(summary, "text", None):
                                summary_parts.append(summary.text)
                        if summary_parts:
                            reasoning_summary = "\n".join(summary_parts)

                elif item_type == "message":
                    # ìµœì¢… ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    if hasattr(item, "content") and item.content:
                        for content in item.content:
                            if getattr(content, "text", None):
                                final_text += content.text

        # fallback to output_text
        if not final_text and hasattr(response, "output_text"):
            final_text = response.output_text or ""

        return GPT5Response(
            text=final_text,
            reasoning_content=reasoning_content,
            reasoning_summary=reasoning_summary,
        )
```

### 5.2 ë°±ì—”ë“œ ìˆ˜ì • - orchestrator.py ë˜ëŠ” event_mapper.py

Reasoningì„ ë³„ë„ ì´ë²¤íŠ¸ë¡œ emit:

```python
# orchestrator.py ë˜ëŠ” agent ì‹¤í–‰ ë¡œì§ì—ì„œ

async def _execute_with_reasoning(self, prompt: str, emit: Callable) -> str:
    provider = get_llm_provider(model=self._model)

    if isinstance(provider, OpenAILLMProvider) and _is_gpt5_model(self._model):
        response = await provider.ainvoke_with_reasoning(prompt)

        # Reasoning ì´ë²¤íŠ¸ emit
        if response.reasoning_content:
            await emit(AgentEvent(
                type=EventType.REASONING,
                subtype=EventSubType.CHUNK,
                content={
                    "reason": response.reasoning_content,  # âœ… í”„ë¡ íŠ¸ì—”ë“œê°€ ê¸°ëŒ€í•˜ëŠ” í•„ë“œ
                    "summary": response.reasoning_summary,
                },
                metadata={"run_id": self._run_id},
            ))

        return response.text
    else:
        return await provider.ainvoke(prompt)
```

### 5.3 í”„ë¡ íŠ¸ì—”ë“œ ìˆ˜ì • - useMultiTabChat.ts (ì„ íƒì )

ë°±ì—”ë“œ ìˆ˜ì • í›„ì—ë„ í˜¸í™˜ì„±ì„ ìœ„í•´ ì—¬ëŸ¬ í•„ë“œ ì§€ì›:

```typescript
// useMultiTabChat.ts:434-442

turn.reasoningText = turn.events
  .filter(event => event.type === 'reasoning')
  .map(event => {
    const content = event.content as any;
    if (!content || typeof content !== 'object') return '';

    // ì—¬ëŸ¬ í•„ë“œ ì§€ì› (í˜¸í™˜ì„±)
    const reasonText = content.reason || content.text || content.reasoning || '';
    return String(reasonText);
  })
  .filter(Boolean)
  .join('\n\n');
```

---

## 6. ì´ë²¤íŠ¸ êµ¬ì¡° í†µì¼ì•ˆ

### 6.1 Reasoning ì´ë²¤íŠ¸ í‘œì¤€ êµ¬ì¡°

```typescript
interface ReasoningEvent {
  type: "reasoning";
  subtype: "start" | "chunk" | "end";
  content: {
    reason: string;      // ì‹¤ì œ reasoning í…ìŠ¤íŠ¸ (í•„ìˆ˜)
    summary?: string;    // reasoning ìš”ì•½ (ì„ íƒ)
    phase?: string;      // ë‹¨ê³„ ì •ë³´ (ì„ íƒ)
  };
  metadata: {
    run_id: string;
  };
  timestamp: string;
}
```

### 6.2 ì´ë²¤íŠ¸ íë¦„ ì˜ˆì‹œ

```
1. reasoning:start  â†’ { phase: "thinking" }
2. reasoning:chunk  â†’ { reason: "Let me analyze...", summary: "Analyzing input" }
3. reasoning:chunk  â†’ { reason: "Step 1: ...", summary: "Step 1 complete" }
4. reasoning:end    â†’ { phase: "complete" }
5. message:chunk   â†’ { text: "Here is my answer..." }
6. message:final   â†’ { text: "Complete response" }
```

---

## 7. êµ¬í˜„ ìš°ì„ ìˆœìœ„

| ìˆœì„œ | ì‘ì—… | íŒŒì¼ | ë³µì¡ë„ |
|------|------|------|--------|
| 1 | GPT-5 ì‘ë‹µì—ì„œ reasoning ì¶”ì¶œ | `providers.py` | ì¤‘ |
| 2 | Reasoning ì´ë²¤íŠ¸ emit | `orchestrator.py` | ì¤‘ |
| 3 | í”„ë¡ íŠ¸ì—”ë“œ í•„ë“œ í˜¸í™˜ì„± | `useMultiTabChat.ts` | ë‚® |
| 4 | Reasoning UI ìŠ¤íƒ€ì¼ë§ | `ReasoningRenderer.tsx` | ë‚® |

---

## 8. í…ŒìŠ¤íŠ¸ ë°©ë²•

### 8.1 ë°±ì—”ë“œ í…ŒìŠ¤íŠ¸

```python
# ì§ì ‘ API í˜¸ì¶œë¡œ reasoning í™•ì¸
import asyncio
from pluto_duck_backend.agent.core.llm.providers import OpenAILLMProvider

async def test_reasoning():
    provider = OpenAILLMProvider(api_key="...", model="gpt-5")
    response = await provider.ainvoke_with_reasoning("Explain quantum computing")
    print("Reasoning:", response.reasoning_content)
    print("Summary:", response.reasoning_summary)
    print("Text:", response.text)

asyncio.run(test_reasoning())
```

### 8.2 í”„ë¡ íŠ¸ì—”ë“œ í…ŒìŠ¤íŠ¸

1. ë¸Œë¼ìš°ì € ê°œë°œì ë„êµ¬ Network íƒ­ì—ì„œ SSE ì´ë²¤íŠ¸ í™•ì¸
2. `type: "reasoning"` ì´ë²¤íŠ¸ì— `content.reason` í•„ë“œê°€ ìˆëŠ”ì§€ í™•ì¸
3. Reasoning UIê°€ ë Œë”ë§ë˜ëŠ”ì§€ í™•ì¸

---

## 9. ì°¸ê³  ìë£Œ

- [OpenAI Responses API Documentation](https://platform.openai.com/docs/api-reference/responses)
- [GPT-5 Reasoning Configuration](https://platform.openai.com/docs/guides/reasoning)
- í”„ë¡œì íŠ¸ ë‚´ ê´€ë ¨ ë¬¸ì„œ:
  - `docs/research/007_chat_ui_vercel_ai_sdk_analysis.md`
  - `docs/research/010_chat_ui_styling_analysis.md`
